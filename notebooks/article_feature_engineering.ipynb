{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272cd2fe",
   "metadata": {},
   "source": [
    "# Article Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd34a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import polars as pl\n",
    "sys.path.append('../') \n",
    "from hnm_data_analysis.feature_engineering import ArticleDescriptionVectoriser\n",
    "from hnm_data_analysis.feature_engineering.articles_text_bert import ArticleDescriptionBertEmbedder\n",
    "from hnm_data_analysis.feature_engineering import CombinedBertArticleFeatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c40052d",
   "metadata": {},
   "source": [
    "### Create a Feature using TF-IDF on detail_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf1ab33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading articles from: ../data/cleaned/articles_cleaned.parquet\n",
      "Articles with valid descriptions: 42,229\n",
      "Prepared cleaned texts: 42,229\n",
      "Fitting TF-IDF: max_features=30000, min_df=5, max_df=0.8, ngram_range=(1, 2)\n",
      "TF-IDF shape: 42,229 docs x 11,309 terms\n",
      "Fitting TruncatedSVD with n_components=200 ...\n",
      "SVD embeddings shape: 42,229 x 200\n",
      "Saving TF-IDF matrix to: ../data/features/tfidf_svd/tfidf_features.npz\n",
      "Saving vectorizer to: ../data/features/tfidf_svd/vectorizer.joblib\n",
      "Saving article_id index to: ../data/features/tfidf_svd/article_id_index.csv\n",
      "Saving SVD embeddings to: ../data/features/tfidf_svd/svd_embeddings.parquet\n",
      "Saving SVD model to: ../data/features/tfidf_svd/svd_model.joblib\n"
     ]
    }
   ],
   "source": [
    "vec = ArticleDescriptionVectoriser(\n",
    "    input_path=\"../data/cleaned/articles_cleaned.parquet\",\n",
    "    language=\"en\",\n",
    "    use_lemmatise=True,\n",
    "    use_stem=False,\n",
    ")\n",
    "tfidf, svd = vec.process(\n",
    "    output_dir=\"../data/features/tfidf_svd\",\n",
    "    include_svd=True,\n",
    "    svd_components=200,\n",
    "    max_features=30000, min_df=5, max_df=0.8, ngram_range=(1,2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a2631",
   "metadata": {},
   "source": [
    "### Create a Feature using BERT on detail_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b128d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading articles from: ../data/cleaned/articles_cleaned.parquet\n",
      "Articles with valid descriptions: 42,229\n",
      "Prepared cleaned texts: 42,229\n",
      "Loading BERT model: sentence-transformers/all-mpnet-base-v2\n",
      "Model loaded on device: cpu\n",
      "Generating BERT embeddings for 42,229 texts...\n",
      "Model: sentence-transformers/all-mpnet-base-v2, Max length: 256, Batch size: 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9ac55b08e34780a1a18fe44315a38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings shape: 42,229 docs x 768 dimensions\n",
      "Fitting PCA with n_components=50 ...\n",
      "PCA embeddings shape: 42,229 x 50\n",
      "Explained variance ratio: 0.793\n",
      "Saving BERT embeddings to: ../data/features/bert/bert_embeddings.parquet\n",
      "Saving model info to: ../data/features/bert/bert_model_info.json\n",
      "Saving article_id index to: ../data/features/bert/article_id_index.csv\n",
      "Saving PCA embeddings to: ../data/features/bert/pca_embeddings.parquet\n",
      "Saving PCA model to: ../data/features/bert/pca_model.joblib\n"
     ]
    }
   ],
   "source": [
    "bert_embedder = ArticleDescriptionBertEmbedder(\n",
    "    input_path=\"../data/cleaned/articles_cleaned.parquet\",\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    max_length=256,\n",
    "    batch_size=16,\n",
    "    device=\"auto\",  # Uses GPU if available\n",
    ")\n",
    "bert_embeddings, pca_embeddings = bert_embedder.process(\n",
    "    output_dir=\"../data/features/bert\",\n",
    "    include_pca=True,\n",
    "    pca_components=50,\n",
    "    pca_normalise=True,\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216246e4",
   "metadata": {},
   "source": [
    "### Combine the BERT Vector Feature with data\\cleaned\\articles_cleaned.parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8739a6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Cleaned articles not found: data/cleaned/articles_last_3_months_cleaned.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Combine the BERT Vector Feature with data\\cleaned\\articles_cleaned.parquet\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Combine cleaned articles + BERT embeddings and save to data/features/combined/\u001b[39;00m\n\u001b[32m      4\u001b[39m job = CombinedBertArticleFeatures()  \u001b[38;5;66;03m# uses module defaults\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m out_path = \u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load and inspect\u001b[39;00m\n\u001b[32m      8\u001b[39m df = pl.read_parquet(out_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analysis Projects/h_and_m_data_analysis/notebooks/../hnm_data_analysis/feature_engineering/combined_features_bert.py:122\u001b[39m, in \u001b[36mCombinedBertArticleFeatures.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     combined = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcombine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save(combined)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analysis Projects/h_and_m_data_analysis/notebooks/../hnm_data_analysis/feature_engineering/combined_features_bert.py:97\u001b[39m, in \u001b[36mCombinedBertArticleFeatures.combine\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcombine\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pl.DataFrame:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.articles_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bert_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    100\u001b[39m     meta_cols = \u001b[38;5;28mself\u001b[39m._select_metadata_columns()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Data Analysis Projects/h_and_m_data_analysis/notebooks/../hnm_data_analysis/feature_engineering/combined_features_bert.py:55\u001b[39m, in \u001b[36mCombinedBertArticleFeatures._load_inputs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(\u001b[38;5;28mself\u001b[39m.cleaned_articles_path):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     56\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCleaned articles not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.cleaned_articles_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m         )\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(\u001b[38;5;28mself\u001b[39m.bert_embeddings_path):\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     60\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBERT embeddings not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.bert_embeddings_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     61\u001b[39m         )\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Cleaned articles not found: data/cleaned/articles_last_3_months_cleaned.parquet"
     ]
    }
   ],
   "source": [
    "os.chdir(\"..\") \n",
    "# Combine the BERT Vector Feature with data\\cleaned\\articles_cleaned.parquet\n",
    "# Combine cleaned articles + BERT embeddings and save to data/features/combined/\n",
    "job = CombinedBertArticleFeatures()  # uses module defaults\n",
    "out_path = job.run()\n",
    "\n",
    "# Load and inspect\n",
    "df = pl.read_parquet(out_path)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
